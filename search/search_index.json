{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Parallel Hyperparameter Tuning","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Parallel hyperparameter tuning using GNU Make.  The similar effect may be achieved with an alternative framework like GNU Parallel, Python standard library <code>multiprocessing</code> module, or the PyTorch Multiprocessing Module <code>torch.multiprocessing</code>.</p> <p>The core idea is analogous to divide and conquer paradigm, just even more banal, and is called scatter and gather.  In context, it means to scatter the large job into smaller, and typically independent, pieces that can run in parallel; and gather them once they\u2019re all finished.</p> <p>The key observation here is that once the data is preprocessed, each training task may be run independently; and the results are collated, once all are finished.  Formally put, say \\(i\\)-th task of \\(N\\) training tasks may be expressed as</p> \\[\\begin{align*} \\rho_i &amp;\\gets \\mathrm{train}(i,\\mathrm{data}) \\\\ \\boldsymbol{\\rho} &amp;\\equiv \\{\\rho_i\\} \\end{align*}\\] <p>In short, the steps are as follows:</p> <ol> <li>Create a search space for hyperparameters;</li> <li>Preprocess: load, sanitise, split, and resave data.</li> <li>Fit the model, evaluate it, and save the pre-trained    model. Do so for each set of hyperparameters;</li> <li>Collate results and deduce the best.</li> </ol>"},{"location":"#usage","title":"Usage","text":"<p><pre><code>make -k dist/hparams.json\nmake -kj 10 all\n</code></pre> Create the dist/hparams.json first, and then run the training, evaluation and collation tasks in parallel, with a bandwidth of 10 processes.</p> <p>In practice, too, the parallelisation resulted in a speed up of \\(10\\times\\) as expected.</p>"},{"location":"#implementation","title":"Implementation","text":"<p>We use the good old GNU Make for this purpose.</p> <p>Make is an order of processes defined in <code>Makefile</code>, so that a dependency graph is inferred and independent processes may be run in parallel.  Once the <code>Makefile</code>, and thereby the dependency graph, is defined by a user, invoking <code>make</code> with the necessary switch automatically runs the independent processes in parallel.</p> <p>The illustrated example is an essential MWE, and also offers a quick refresher of GNU Make.  The actual implementation is slightly more involved, and in the spirit of DRY/DIE.</p>"},{"location":"#targets-and-recipes","title":"Targets and Recipes","text":"<p>Consider the following <code>Makefile</code>,</p> <pre><code># Makefile\nall : dist/.collated\ndist/.collated : dist/.trained \n    python -m collate\n    touch dist/.collated\ndist/.trained : dist/trained/A/.trained dist/trained/B/.trained\n    touch dist/.trained\ndist/trained/A/.trained : dist/preprocessed\n    python -m train\n    touch dist/trained/A/.trained\ndist/trained/B/.trained : dist/preprocessed\n    python -m train\n    touch dist/trained/B/.trained\n# ...and so forth\n</code></pre> <p>It consists of a set of relationships of the form:</p> <pre><code>target : [ dependencies ]\n       [ recipe ]\n</code></pre> <p>Each <code>target</code> is a filename (or sometimes not). <code>recipe</code> is a set of shell commands that are responsible to create the <code>target</code>.  <code>dependencies</code> are prerequisites, such that only after ensuring that they are up-to-date, the recipe for a target is invoked.</p> <p>In the illustrated Makefile, the first rule says, target <code>all</code> is satisfied if <code>dist/.collated</code> is.</p> <p>The second says, <code>dist/.collated</code> is satisfied if <code>dist/.trained</code> is and the subsequent recipe runs without error.  It\u2019s recipe may be understood as invoking python CLI module <code>collate</code> and thus updating (or creating) the target file explicitly.</p> <p>Similarly the third fourth and fifth targets define how the target <code>dist/.trained</code> are defined.</p> <p>Once again, the illustrated example is an essential MWE.  The actual implementation is slightly more involved, and in the spirit of DRY/DIE.</p>"},{"location":"#invocation","title":"Invocation","text":"<p>A target may be invoked from command-line using</p> <pre><code>make [OPTIONS] [TARGET] [VAR=VAL]\n</code></pre> <p>If unspecified the first target defined in the <code>Makefile</code> is the default.</p> <p>Commonly used <code>[OPTIONS]</code> include</p> <ul> <li><code>-n</code> to dry run;</li> <li><code>-B</code> to always make;</li> <li><code>-k</code> to keep going as far as possible (even after error);</li> <li><code>-f</code> to specify Makefile; </li> <li><code>-C</code> to change directory;</li> <li><code>-j</code> for number of parallel jobs;</li> <li><code>-i</code> to ignore errors.</li> </ul> <p>Further Reading: GNU Make Manual</p>"},{"location":"phs/","title":"Phs","text":""},{"location":"phs/#phs","title":"<code>phs</code>","text":"<p>Parallel Hyperparameter Search Module</p> <p>This module is primarily governed by four CLI scripts, namely, <code>generateHparams</code>, <code>preprocess</code>, <code>train</code> and <code>collate</code>.</p> <p>Modules:</p> Name Description <code>datasetFunctions</code> <code>trainFunctions</code> <p>Attributes:</p> Name Type Description <code>VERSION</code>"},{"location":"phs/#phs.VERSION","title":"<code>VERSION = '1.0.1'</code>  <code>module-attribute</code>","text":""},{"location":"phs/datasetFunctions/","title":"datasetFunctions","text":""},{"location":"phs/datasetFunctions/#phs.datasetFunctions","title":"<code>phs.datasetFunctions</code>","text":"<p>Functions:</p> Name Description <code>loadData</code> <p>Load the four datasets</p> <code>readDataFromZipArchive</code> <p>Read a Pandas dataframe from excel file <code>FNAME</code></p> <code>sanitiseData</code> <p>Sanitise Raw Data <code>instanceof(pandas.DataFrame)</code>.</p> <code>saveData</code> <p>Save the four datasets with filenames based on</p> <code>splitData</code> <p>Split <code>dataClean</code> into trainVal and test sets.</p> <p>Attributes:</p> Name Type Description <code>fnamesFmt</code>"},{"location":"phs/datasetFunctions/#phs.datasetFunctions.fnamesFmt","title":"<code>fnamesFmt = ['{prefix}/xTrainVal.txt', '{prefix}/yTrainVal.txt', '{prefix}/xTest.txt', '{prefix}/yTest.txt']</code>  <code>module-attribute</code>","text":""},{"location":"phs/datasetFunctions/#phs.datasetFunctions.loadData","title":"<code>loadData(prefix)</code>","text":"<p>Load the four datasets <code>xTrainVal,yTrainVal,xTest,yTest</code> from filenames based on format <code>fnamesFmt</code>.</p> <p>Convert the datasets to numpy.</p>"},{"location":"phs/datasetFunctions/#phs.datasetFunctions.readDataFromZipArchive","title":"<code>readDataFromZipArchive(archive, fname)</code>","text":"<p>Read a Pandas dataframe from excel file <code>FNAME</code> within <code>ARCHIVE</code>.</p>"},{"location":"phs/datasetFunctions/#phs.datasetFunctions.sanitiseData","title":"<code>sanitiseData(dataRaw)</code>","text":"<p>Sanitise Raw Data <code>instanceof(pandas.DataFrame)</code>.</p> <ol> <li>Coerce Errors.</li> <li>Drop columns with more than 50% missing values.</li> <li>Drop rows with NA.</li> <li>Retrieve \\(Y\\) and \\(X\\) as numpy arrays.</li> <li>Transform \\(Y\\)'s... Here \\(Y \\in \\{-1,1\\}\\) instead of \\(\\{0,1\\}\\).</li> </ol>"},{"location":"phs/datasetFunctions/#phs.datasetFunctions.saveData","title":"<code>saveData(prefix, xTrainVal, yTrainVal, xTest, yTest)</code>","text":"<p>Save the four datasets with filenames based on format <code>fnamesFmt</code>.</p>"},{"location":"phs/datasetFunctions/#phs.datasetFunctions.splitData","title":"<code>splitData(dataClean, trainValFactor=0.8)</code>","text":"<p>Split <code>dataClean</code> into trainVal and test sets.</p>"},{"location":"phs/trainFunctions/","title":"trainFunctions","text":""},{"location":"phs/trainFunctions/#phs.trainFunctions","title":"<code>phs.trainFunctions</code>","text":"<p>Classes:</p> Name Description <code>Experts</code> <p>Convenience Class for a Mixture of Experts Classifier.</p> <code>Ksplit</code> <p>Convenience Class k-Split (for k-fold CV).</p> <p>Functions:</p> Name Description <code>getTrainedModel</code> <p>Train a model using Linear SVM Classifier.</p>"},{"location":"phs/trainFunctions/#phs.trainFunctions.Experts","title":"<code>Experts</code>  <code>dataclass</code>","text":"<p>Convenience Class for a Mixture of Experts Classifier.</p> <p>Methods:</p> Name Description <code>__get__</code> <p>Predict the class of X based on \"opinion\" of</p> <code>__getitem__</code> <p>Convenience subscript access for experts.</p> <code>acc</code> <p>Convenience function of na\u00efve accuracy</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>list[int]</code> <code>experts</code> <code>list[dict]</code>"},{"location":"phs/trainFunctions/#phs.trainFunctions.Experts.classes","title":"<code>classes = field(default_factory=lambda: [0, 1])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"phs/trainFunctions/#phs.trainFunctions.Experts.experts","title":"<code>experts</code>  <code>instance-attribute</code>","text":""},{"location":"phs/trainFunctions/#phs.trainFunctions.Experts.__get__","title":"<code>__get__(X)</code>","text":"<p>Predict the class of X based on \"opinion\" of experts. <code>Y = self(X)</code></p>"},{"location":"phs/trainFunctions/#phs.trainFunctions.Experts.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Convenience subscript access for experts.</p> <p><code>self[i]</code> is the same as <code>self.experts[i]</code></p>"},{"location":"phs/trainFunctions/#phs.trainFunctions.Experts.acc","title":"<code>acc(X, Y)</code>","text":"<p>Convenience function of na\u00efve accuracy calculator.</p> <p><code>acc = np.mean(self(X)==Y)</code></p>"},{"location":"phs/trainFunctions/#phs.trainFunctions.Ksplit","title":"<code>Ksplit</code>  <code>dataclass</code>","text":"<p>Convenience Class k-Split (for k-fold CV).</p> <p>Split a dataset with \\(N\\) samples into \\(k\\) parts. Only maintain indices so that <code>ksplit[i]</code> provides a set of indices subscriptable to the original dataset, e.g. <code>iTrain, iVal = ksplit[i]; xTrain = X[iTrain]</code> creates a train data subset for <code>i</code>-th fold of validation.</p> <p>On initialisation, create a random shuffled set of indices if not already provided.</p> <p><code>[i]</code> subscript access will fetch the <code>i</code>-th pair of indices corresponding to train and val split respectively.</p> <p>Attributes:</p> Name Type Description <code>N</code> <code>int</code> <code>indices</code> <code>list[int]</code> <code>k</code> <code>int</code>"},{"location":"phs/trainFunctions/#phs.trainFunctions.Ksplit.N","title":"<code>N</code>  <code>instance-attribute</code>","text":""},{"location":"phs/trainFunctions/#phs.trainFunctions.Ksplit.indices","title":"<code>indices = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"phs/trainFunctions/#phs.trainFunctions.Ksplit.k","title":"<code>k</code>  <code>instance-attribute</code>","text":""},{"location":"phs/trainFunctions/#phs.trainFunctions.getTrainedModel","title":"<code>getTrainedModel(xTrain, yTrain, xVal, yVal, hparams)</code>","text":"<p>Train a model using Linear SVM Classifier.</p> <p><code>hparams</code> is a dict with key <code>'C'</code></p>"},{"location":"phs/cli/","title":"CLI Index","text":"<p>Command-line interface scripts available in this module.</p> <ul> <li>train</li> <li>preprocess</li> <li>collate</li> <li>generateHparams</li> </ul>"},{"location":"phs/cli/collate/","title":"collate","text":"<p>Usage:</p> <pre><code>collate [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --out-path FILE\n  --hparams-path FILE\n  --train-dir DIRECTORY\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"phs/cli/generateHparams/","title":"generateHparams","text":"<p>Generate Hparams.</p> <p>Write the generated <code>pandas.DataFrame</code> to output file <code>OUT</code> in json format.</p> <p>Given \\(N\\), \\(c_{\\text{start}}\\), \\(c_{\\text{range}}\\), \\(k\\); (\\(k\\) as in \\(k\\)-fold cross validation) Generate a csv file with \\(N\\) records and header: <code>ID,C,k</code> such that \\(C = 10^{c_{10}}; c_{10} = c_{\\text{start}} + c_{\\text{range}} \\cdot u; u\\sim\\mathcal{U}(0,1)\\) and ID is a 4-char long random hex.</p> <p>In order to adapt this, modify the click command, arguments and options.</p> <p>Usage:</p> <pre><code>generateHparams [OPTIONS] OUT N CSTART CRANGE K\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"phs/cli/preprocess/","title":"preprocess","text":"<p>Preprocess the database.  Following is the comprehensive subroutine in essence.  Note that the output of any of the functions has not been bound to a specific type here.  They are entirely dependent upon the module <code>phs.datasetFunctions</code> to enforce, where the four functions are defined.</p> <pre><code>dataRaw = readDataFromZipArchive(archive, archive_fname)\ndataClean = sanitiseData(dataRaw)\nxTrainVal,yTrainVal,xTest,yTest = splitData(dataClean)\nsaveData(data_dir,xTrainVal,yTrainVal,xTest,yTest)\n</code></pre> <p>Usage:</p> <pre><code>preprocess [OPTIONS] ARCHIVE ARCHIVE_FNAME\n</code></pre> <p>Options:</p> <pre><code>  -D, --data-dir DIRECTORY  Data Directory.\n  --help                    Show this message and exit.\n</code></pre>"},{"location":"phs/cli/train/","title":"train","text":"<p>Train the model with given hparams (<code>HPARAMS_PATH</code>, <code>ID</code>) using data from <code>DATA_DIR</code> and finally save the metadata into <code>OUT_DIR</code>/<code>RESULT_FNAME</code> and the model itself into <code>OUT_DIR</code>/<code>MODEL_FNAME</code>.</p> <p>The hparams is a look-up performed over col <code>id</code> bearing value <code>ID</code> (as in command line option <code>--id</code>) within the <code>pandas.DataFrame</code> read from the json file <code>HPARAMS_PATH</code>.</p> <p>The data is loaded using <code>phs.datasetFunctions.loadData</code> with argument <code>DATA_DIR</code>, and return a tuple <code>xTrainVal,yTrainVal,xTest,yTest</code>.  Data sanitisation is taken care of in the pre-process step.</p> <p><code>phs.trainFunctions.Ksplit</code> is responsible for shuffling and splitting the indices into train and val.</p> <p><code>phs.trainFunctions.getTrainedModel</code> is responsible for training the model.  It returns a model with metadata of the form:</p> <pre><code>{ \"model\": \"...&lt;the python model&gt;...\", \n  \"hparams\": {\"C\": 0.7073982632},\n  \"metrics\": {\"valAcc\": 0.8333333333333334},\n  \"theMetric\": \"valAcc\"\n}\n</code></pre> <p>The collation is based on the value of the key in dict <code>metrics</code> given by <code>theMetric</code>.</p> <p>Usage:</p> <pre><code>train [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --out-dir DIRECTORY   [required]\n  --result-fname TEXT   [required]\n  --model-fname TEXT    [required]\n  --hparams-path FILE   [required]\n  --data-dir DIRECTORY  [required]\n  --id TEXT             [required]\n  --help                Show this message and exit.\n</code></pre>"}]}